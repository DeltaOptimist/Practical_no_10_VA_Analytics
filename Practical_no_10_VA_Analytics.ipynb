{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndHkAEAkVLbw"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision yolov5 deep_sort_realtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from yolov5 import YOLOv5\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "# Import the necessary function for displaying images in Colab\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize YOLOv5 model\n",
        "model_path = 'yolov5s.pt'  # Adjust path to your YOLOv5 model file\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "yolo_model = YOLOv5(model_path, device)\n",
        "\n",
        "# Initialize Deep SORT tracker\n",
        "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
        "\n",
        "# Open video file\n",
        "cap = cv2.VideoCapture('/content/5538137-hd_1920_1080_25fps.mp4')\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run object detection\n",
        "    results = yolo_model.predict(frame)\n",
        "\n",
        "    # Prepare detections for the tracker\n",
        "    detections = []\n",
        "    for *xyxy, conf, cls in results.pred[0]:\n",
        "        bbox = [int(xyxy[0]), int(xyxy[1]), int(xyxy[2]) - int(xyxy[0]), int(xyxy[3]) - int(xyxy[1])]\n",
        "        detections.append((bbox, conf, cls))\n",
        "\n",
        "    # Pass detections to the tracker\n",
        "    tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "    # Loop over the tracks and draw boxes with IDs\n",
        "    for track in tracks:\n",
        "        if not track.is_confirmed():\n",
        "            continue\n",
        "        track_id = track.track_id\n",
        "        bbox = track.to_tlbr()  # Get bounding box in (x1, y1, x2, y2) format\n",
        "\n",
        "        # Draw bounding box and label on the frame\n",
        "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"ID: {track_id}\", (int(bbox[0]), int(bbox[1]) - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame using cv2_imshow instead of cv2.imshow\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "5TdJPJ7zWJg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries:\n",
        "\n",
        "cv2: OpenCV library for handling video and image processing.\n",
        "\n",
        "torch: PyTorch library to utilize CUDA (GPU) if available.\n",
        "\n",
        "YOLOv5: The YOLOv5 model for object detection.\n",
        "\n",
        "DeepSort: Multi-object tracking class that uses a deep association metric for tracking.\n",
        "\n",
        "cv2_imshow: Colab-specific function for displaying frames.\n",
        "\n",
        "\n",
        "**Load YOLOv5 Model:**\n",
        "\n",
        "\n",
        "model_path = 'yolov5s.pt'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "yolo_model = YOLOv5(model_path, device)\n",
        "\n",
        "\n",
        "model_path: Specifies the path to the pretrained YOLOv5 model file (yolov5s.pt), which is used for detecting objects in each frame.\n",
        "\n",
        "device: Determines whether to use the GPU (cuda) or CPU (cpu) based on availability.\n",
        "\n",
        "yolo_model: Instantiates the YOLOv5 model on the specified device.\n",
        "\n",
        "\n",
        "**Initialize Deep SORT Tracker:**\n",
        "\n",
        "\n",
        "tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)\n",
        "\n",
        "max_age: The maximum number of frames a track can be missing before it is deleted.\n",
        "\n",
        "n_init: Minimum number of frames for a new detection to be confirmed as a track.\n",
        "\n",
        "nn_budget: Limits the size of the embedding gallery to control memory usage.\n",
        "\n",
        "\n",
        "**Open the Video File:**\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture('/content/5538137-hd_1920_1080_25fps.mp4')\n",
        "\n",
        "cap: Video capture object to read the video file frame-by-frame.\n",
        "\n",
        "You can replace the path with any video file in your Colab workspace."
      ],
      "metadata": {
        "id": "Wuf5V6XCVM-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process Each Frame:**\n",
        "\n",
        "The while cap.isOpened() loop ensures the video is open and processes each frame in sequence.\n",
        "\n",
        "ret, frame = cap.read(): Reads each frame of the video.\n",
        "\n",
        "if not ret: break: Exits the loop if there are no more frames.\n"
      ],
      "metadata": {
        "id": "2zwqasNuXBZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Object Detection:**\n",
        "\n",
        "\n",
        "results = yolo_model.predict(frame)\n",
        "\n",
        "Detects objects in the current frame and stores the results, which include bounding box coordinates, confidence scores, and class labels."
      ],
      "metadata": {
        "id": "lEH6EF19XGj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Detections for Tracking:**\n",
        "\n",
        "\n",
        "detections = []\n",
        "for *xyxy, conf, cls in results.pred[0]:\n",
        "    bbox = [int(xyxy[0]), int(xyxy[1]), int(xyxy[2]) - int(xyxy[0]), int(xyxy[3]) - int(xyxy[1])]\n",
        "    detections.append((bbox, conf, cls))\n",
        "\n",
        "Extracts detection information from results.pred[0].\n",
        "\n",
        "bbox: Bounding box coordinates are formatted as (x1, y1, width, height) for compatibility with the tracker.\n",
        "\n",
        "detections: Collects bounding boxes, confidence scores, and class labels for each detected object."
      ],
      "metadata": {
        "id": "1CW3MOPaXPc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Track Objects Using Deep SORT:**\n",
        "\n",
        "\n",
        "tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "update_tracks: Deep SORT updates each objectâ€™s position and maintains an ID for each tracked object across frames.\n",
        "\n",
        "\n",
        "**Draw Bounding Boxes and Track IDs:**\n",
        "\n",
        "\n",
        "for track in tracks:\n",
        "    if not track.is_confirmed():\n",
        "        continue\n",
        "    track_id = track.track_id\n",
        "    bbox = track.to_tlbr()  # Get bounding box in (x1, y1, x2, y2) format\n",
        "\n",
        "    # Draw bounding box and label on the frame\n",
        "    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
        "    cv2.putText(frame, f\"ID: {track_id}\", (int(bbox[0]), int(bbox[1]) - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "For each confirmed track, the code draws a bounding box and assigns a unique ID to each object.\n",
        "\n",
        "track.to_tlbr(): Converts the bounding box format to (x1, y1, x2, y2) for use with OpenCV drawing functions.\n",
        "\n",
        "**Display Frame with Detections and Tracks:**\n",
        "\n",
        "\n",
        "cv2_imshow(frame)\n",
        "\n",
        "Displays the processed frame with bounding boxes and object IDs in Colab.\n",
        "\n",
        "**Exit Condition:**\n",
        "\n",
        "\n",
        "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    break\n",
        "\n",
        "Allows the user to quit the video processing loop by pressing the 'q' key (primarily for desktop environments, not necessary in Colab).\n",
        "\n",
        "**Release Video Capture and Close Windows:**\n",
        "\n",
        "\n",
        "cap.release()\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "Ensures resources are released and any OpenCV windows are closed (mainly for non-Colab environments).\n",
        "\n"
      ],
      "metadata": {
        "id": "HuU7a_RhXXa5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CjsA4w7JXE_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}